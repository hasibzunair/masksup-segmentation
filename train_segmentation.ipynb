{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "#os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(0)\n",
    "os.environ['PYTHONHASHSEED'] = str(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "EXPERIMENT_NAME = \"unet_isic2018\"\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "LOG_PATH = os.path.join(ROOT_DIR, \"logs\", EXPERIMENT_NAME)\n",
    "\n",
    "if not os.path.exists(os.path.join(ROOT_DIR, \"logs\")):\n",
    "    os.mkdir(os.path.join(ROOT_DIR, \"logs\"))\n",
    "    \n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data\n",
    "\n",
    "Download ISIC-2018 lesion segmentation dataset from https://challenge.isic-archive.com/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ISIC2018_dataloader(Dataset):\n",
    "    def __init__(self, data_folder, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        self._data_folder = data_folder\n",
    "        self.build_dataset()\n",
    "\n",
    "    def build_dataset(self):\n",
    "        self._input_folder = os.path.join(self._data_folder, 'ISIC2018_Task1-2_Training_Input')\n",
    "        self._label_folder = os.path.join(self._data_folder, 'ISIC2018_Task1_Training_GroundTruth')\n",
    "        self._images = sorted(glob.glob(self._input_folder + \"/*.jpg\"))\n",
    "        self._labels = sorted(glob.glob(self._label_folder + \"/*.png\"))\n",
    "        \n",
    "        self.train_images, self.test_images, self.train_labels, self.test_labels = train_test_split(self._images, self._labels, \n",
    "                                                                            test_size=0.2, shuffle=False, random_state=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.train_images)\n",
    "        else:\n",
    "            return len(self.test_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.is_train:\n",
    "            img_path = self.train_images[idx]\n",
    "            mask_path = self.train_labels[idx]\n",
    "        else:\n",
    "            img_path = self.test_images[idx]\n",
    "            mask_path = self.test_labels[idx]\n",
    "            \n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('P')\n",
    "        \n",
    "        transforms_image = transforms.Compose([transforms.Resize((256, 256)), transforms.CenterCrop((256,256)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))])\n",
    "        \n",
    "        transforms_mask = transforms.Compose([transforms.Resize((256, 256)), transforms.CenterCrop((256,256)),\n",
    "                                             transforms.ToTensor()])\n",
    "        \n",
    "        image = transforms_image(image)\n",
    "        mask = transforms_mask(mask)\n",
    "        \n",
    "        sample = {'image': image, 'mask': mask}\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "\n",
    "train_dataset = ISIC2018_dataloader(\"datasets/ISIC2018\")\n",
    "test_dataset = ISIC2018_dataloader(\"datasets/ISIC2018\", is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = next(iter(train_dataloader))\n",
    "x = dt[\"image\"]\n",
    "y = dt[\"mask\"]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(ten):\n",
    "    ten =(ten[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    ten=(ten*255).astype(np.uint8)\n",
    "    return ten\n",
    "\n",
    "a = to_img(x)\n",
    "print(a.shape)\n",
    "plt.imshow(a)\n",
    "#plt.imshow(a, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = to_img(y)\n",
    "print(a.shape)\n",
    "plt.imshow(a, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "###### UNet model\n",
    "#########################################################################################\n",
    "\"\"\" Convolutional block:\n",
    "    It follows a two 3x3 convolutional layer, each followed by a batch normalization and a relu activation.\n",
    "\"\"\"\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\"\"\" Encoder block:\n",
    "    It consists of an conv_block followed by a max pooling.\n",
    "    Here the number of filters doubles and the height and width half after every block.\n",
    "\"\"\"\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p\n",
    "\n",
    "\"\"\" Decoder block:\n",
    "    The decoder block begins with a transpose convolution, followed by a concatenation with the skip\n",
    "    connection from the encoder block. Next comes the conv_block.\n",
    "    Here the number filters decreases by half and the height and width doubles.\n",
    "\"\"\"\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "        \n",
    "        # NOTE: \n",
    "        # nn.Conv2d(64, 1, kernel_size=1, padding=0) is mathematically same as \n",
    "        # nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs\n",
    "######################################################################################### \n",
    "\n",
    "\n",
    "# Define model\n",
    "model = build_unet()\n",
    "\n",
    "# Send to GPU\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All parameters\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"All parameters \", all_params)\n",
    "\n",
    "# Trainable parameters\n",
    "all_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable parameters \", all_train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup optim and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss() # loss combines a Sigmoid layer and the BCELoss in one single class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        data, target = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE)\n",
    "        output = model.forward(data.float())\n",
    "        loss = criterion(output.float(), target.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "        #         100. * batch_idx / len(train_dataloader), loss.data))\n",
    "            \n",
    "def test(model):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        jaccard = 0\n",
    "        dice = 0\n",
    "\n",
    "        for data in test_dataloader:\n",
    "            data, target = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE)\n",
    "            output = model(data.float())  \n",
    "            test_loss += criterion(output.float(), target.float()).item()\n",
    "            \n",
    "            output = torch.sigmoid(output) # Turn activations into probabilities by feeding through sigmoid\n",
    "            gt = target.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy()\n",
    "            pred = output.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() > 0.5\n",
    "\n",
    "            intersection = pred * gt\n",
    "            union = pred + gt - intersection\n",
    "            jaccard += (np.sum(intersection)/np.sum(union))  \n",
    "            dice += (2. * np.sum(intersection) ) / (np.sum(pred) + np.sum(gt))\n",
    "    \n",
    "        test_loss /= len(test_dataloader)\n",
    "        jaccard /= len(test_dataloader)\n",
    "        dice /= len(test_dataloader)\n",
    "\n",
    "        losses.append(test_loss)\n",
    "        jacs.append(jaccard)\n",
    "        dices.append(dice)\n",
    "\n",
    "\n",
    "        print('Average Loss: {:.3f}'.format(test_loss))\n",
    "        print('Jaccard Index : {:.3f}'.format(jaccard * 100))\n",
    "        print('Dice Coefficient : {:.3f}'.format(dice * 100))\n",
    "        print('==========================================')\n",
    "        print('==========================================')\n",
    "        return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "jacs = []\n",
    "dices = []\n",
    "\n",
    "score = 0\n",
    "best_score = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "N_EPOCHS = 5 # Do 100 or more!\n",
    " \n",
    "for epoch in range(1, N_EPOCHS):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    train(model, epoch)\n",
    "    score = test(model)\n",
    "    \n",
    "    # Save best model\n",
    "    if score > best_score:\n",
    "        print(\"Saving model at dice={:.3f}\".format(score))\n",
    "        torch.save(model.state_dict(), '{}/{}.pth'.format(LOG_PATH, EXPERIMENT_NAME))\n",
    "        best_score = score\n",
    "\n",
    "        \n",
    "# Save losses\n",
    "losses = np.array(losses)\n",
    "np.savetxt(\"{}/{}_loss.txt\".format(LOG_PATH, EXPERIMENT_NAME), losses, delimiter=\",\")\n",
    "jacs = np.array(jacs)\n",
    "np.savetxt(\"{}/{}_jacs.txt\".format(LOG_PATH, EXPERIMENT_NAME), jacs, delimiter=\",\")\n",
    "dices = np.array(dices)\n",
    "np.savetxt(\"{}/{}_dices.txt\".format(LOG_PATH, EXPERIMENT_NAME), dices, delimiter=\",\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))\n",
    "print(\"--- Time taken to train : %s mins ---\" % ((end_time - start_time)//60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(jacs), max(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "# b, g, r, y, o, -g, -m,\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(losses,linewidth=4)\n",
    "plt.title('{} loss'.format(\"Exp name\"))\n",
    "#plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['loss'], loc='upper left')\n",
    "plt.grid(True)\n",
    "# Plot training & validation iou_score values\n",
    "plt.subplot(122)\n",
    "plt.plot(jacs,linewidth=4)\n",
    "plt.plot(dices,linewidth=4)\n",
    "#plt.title('{} IOU score'.format(experiment_name))\n",
    "#plt.ylabel('iou_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend(['Jaccard', 'Dice'], loc='upper left')\n",
    "plt.savefig('{}/{}_graph.png'.format(LOG_PATH, EXPERIMENT_NAME), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
