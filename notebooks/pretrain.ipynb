{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train COIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(0)\n",
    "os.environ['PYTHONHASHSEED'] = str(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "EXPERIMENT_NAME = \"coin\"\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "LOG_PATH = os.path.join(ROOT_DIR, \"logs\", EXPERIMENT_NAME)\n",
    "\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)\n",
    "    \n",
    "if not os.path.exists(os.path.join(LOG_PATH, \"outs\")):\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\", \"preds\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\", \"partials\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\", \"inputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hasib/context-invariance/logs/coin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\" \n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).lower(), fontsize=20)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        #plt.tight_layout()\n",
    "    #plt.savefig(\"../outputs/vis/compare-segs/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from dataset import ISIC2018Masked_dataloader\n",
    "\n",
    "train_dataset = ISIC2018Masked_dataloader(\"datasets/ISIC2018\")\n",
    "test_dataset = ISIC2018Masked_dataloader(\"datasets/ISIC2018\", is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = next(iter(train_dataloader))\n",
    "# x = dt[\"image\"]\n",
    "# y = dt[\"mask\"]\n",
    "# z = dt[\"partial_image1\"]\n",
    "# z2 = dt[\"partial_image2\"]\n",
    "# x.shape, y.shape, z.shape, z2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_img(ten):\n",
    "#     ten =(ten[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     ten=(ten*255).astype(np.uint8)\n",
    "#     return ten\n",
    "\n",
    "# a = to_img(x)\n",
    "# print(a.shape)\n",
    "# #plt.imshow(a)\n",
    "# plt.imshow(a, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xs, ys, zs, zs2 in zip(x,y,z,z2):\n",
    "    \n",
    "#     p_in = (zs.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     p_in = (p_in*255).astype(np.uint8)\n",
    "    \n",
    "#     p_in2 = (zs2.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     p_in2 = (p_in2*255).astype(np.uint8)\n",
    "    \n",
    "#     # Preprocess Input\n",
    "#     xs = (xs.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     xs = (xs*255).astype(np.uint8)\n",
    "    \n",
    "#     # Preprocess Mask\n",
    "#     ys =(ys.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     ys = (ys*255).astype(np.uint8)\n",
    "    \n",
    "#     visualize(0, input_image=xs, mask=ys, partial_input=p_in, partial_input_2=p_in2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COIN(\n",
       "  (input_block): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (input_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bridge): Bridge(\n",
       "    (bridge): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(2048, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(67, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import * \n",
    "\n",
    "# Define model\n",
    "model = COIN()\n",
    "\n",
    "# Send to GPU and initialize weights\n",
    "model = model.to(DEVICE)\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup optim and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion_mae = torch.nn.L1Loss() # MAE, L1\n",
    "criterion_mse = torch.nn.MSELoss() # MSE, L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        data, mask, partial1, partial2 = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE), data[\"partial_image1\"].to(DEVICE), data[\"partial_image2\"].to(DEVICE)\n",
    "        \n",
    "        # Pred\n",
    "        output1, output2 = model.forward(partial1.float(), partial2.float())\n",
    "        \n",
    "        # Post process\n",
    "        output1 = output1[:,0:3,:,:]\n",
    "        output1 = torch.tanh(output1)\n",
    "        #output2 = output2[:,0:3,:,:]\n",
    "        #output2 = torch.tanh(output2)\n",
    "        \n",
    "        # One way loss\n",
    "        loss = 0.9 * criterion_mse(output1.float(), data.float()) + 0.1 * criterion_mae(output1.float(), data.float())\n",
    "        \n",
    "        # Two way loss\n",
    "        #loss = 0.9 * criterion_mse(output1.float(), data.float()) + 0.1 * criterion_mae(output1.float(), data.float())\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ######################\n",
    "        if batch_idx == 0:\n",
    "            # preds\n",
    "            cv_img=(output1[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "            rgb=(cv_img*255).astype(np.uint8)\n",
    "            bgr=cv2.cvtColor(rgb,cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite('{}/outs/preds/test'.format(LOG_PATH)+str(epoch)+'.jpg',bgr)\n",
    "\n",
    "            # partial inputs\n",
    "            xx=(partial1[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "            xx=(xx*255).astype(np.uint8)\n",
    "            xx=cv2.cvtColor(xx,cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite('{}/outs/partials/test'.format(LOG_PATH)+str(epoch)+'.jpg',xx)\n",
    "\n",
    "            # inputs\n",
    "            yy=(data[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "            yy=(yy*255).astype(np.uint8)\n",
    "            yy=cv2.cvtColor(yy,cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite('{}/outs/inputs/test'.format(LOG_PATH)+str(epoch)+'.jpg',yy)\n",
    "        ######################\n",
    "        \n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "        #         100. * batch_idx / len(train_dataloader), loss.data))\n",
    "        \n",
    "    train_loss /= len(train_dataloader) \n",
    "    train_losses.append(train_loss)\n",
    "    print('Average training loss: {:.3f}'.format(train_loss))\n",
    "\n",
    "\n",
    "            \n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(train_dataloader):\n",
    "            data, mask, partial = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE), data[\"partial_image1\"].to(DEVICE)\n",
    "            \n",
    "            # Pred\n",
    "            output, _ = model.forward(partial.float(), partial.float())\n",
    "            output = output[:,0:3,:,:]\n",
    "            output = torch.tanh(output)\n",
    "            # Compute loss\n",
    "            test_loss += criterion_mse(output.float(), data.float()).item()\n",
    "        \n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print('Average test loss: {:.3f}'.format(test_loss))\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average training loss: 0.076\n",
      "Average test loss: 0.047\n",
      "Saving model ------------------------------ at loss = 0.047.\n",
      "Epoch: 2\n",
      "Average training loss: 0.049\n",
      "Average test loss: 0.042\n",
      "Saving model ------------------------------ at loss = 0.042.\n",
      "Epoch: 3\n",
      "Average training loss: 0.046\n",
      "Average test loss: 0.042\n",
      "Saving model ------------------------------ at loss = 0.042.\n",
      "Epoch: 4\n",
      "Average training loss: 0.044\n",
      "Average test loss: 0.046\n",
      "Epoch: 5\n",
      "Average training loss: 0.041\n",
      "Average test loss: 0.139\n",
      "Epoch: 6\n",
      "Average training loss: 0.040\n",
      "Average test loss: 0.128\n",
      "Epoch: 7\n",
      "Average training loss: 0.037\n",
      "Average test loss: 0.194\n",
      "Epoch: 8\n",
      "Average training loss: 0.037\n",
      "Average test loss: 0.133\n",
      "Epoch: 9\n",
      "Average training loss: 0.037\n",
      "Average test loss: 0.210\n",
      "Epoch: 10\n",
      "Average training loss: 0.036\n",
      "Average test loss: 0.035\n",
      "Saving model ------------------------------ at loss = 0.035.\n",
      "Epoch: 11\n",
      "Average training loss: 0.035\n",
      "Average test loss: 0.164\n",
      "Epoch: 12\n",
      "Average training loss: 0.034\n",
      "Average test loss: 0.090\n",
      "Epoch: 13\n",
      "Average training loss: 0.034\n",
      "Average test loss: 0.033\n",
      "Saving model ------------------------------ at loss = 0.033.\n",
      "Epoch: 14\n",
      "Average training loss: 0.034\n",
      "Average test loss: 0.067\n",
      "Epoch: 15\n",
      "Average training loss: 0.034\n",
      "Average test loss: 0.031\n",
      "Saving model ------------------------------ at loss = 0.031.\n",
      "Epoch: 16\n",
      "Average training loss: 0.033\n",
      "Average test loss: 0.031\n",
      "Epoch: 17\n",
      "Average training loss: 0.032\n",
      "Average test loss: 0.027\n",
      "Saving model ------------------------------ at loss = 0.027.\n",
      "Epoch: 18\n",
      "Average training loss: 0.032\n",
      "Average test loss: 0.053\n",
      "Epoch: 19\n",
      "Average training loss: 0.031\n",
      "Average test loss: 0.048\n",
      "Epoch: 20\n",
      "Average training loss: 0.032\n",
      "Average test loss: 0.171\n",
      "Epoch: 21\n",
      "Average training loss: 0.032\n",
      "Average test loss: 0.035\n",
      "Epoch: 22\n",
      "Average training loss: 0.031\n",
      "Average test loss: 0.075\n",
      "Epoch: 23\n",
      "Average training loss: 0.031\n",
      "Average test loss: 0.152\n",
      "Epoch: 24\n",
      "Average training loss: 0.031\n",
      "Average test loss: 0.029\n",
      "Epoch: 25\n",
      "Average training loss: 0.030\n",
      "Average test loss: 0.110\n",
      "Epoch: 26\n",
      "Average training loss: 0.030\n",
      "Average test loss: 0.036\n",
      "Epoch: 27\n",
      "Average training loss: 0.031\n",
      "Average test loss: 0.033\n",
      "Epoch: 28\n",
      "Average training loss: 0.031\n",
      "Average test loss: 0.033\n",
      "Epoch: 29\n",
      "Average training loss: 0.030\n",
      "Average test loss: 0.034\n",
      "Epoch: 30\n",
      "Average training loss: 0.030\n",
      "Average test loss: 0.033\n",
      "Epoch: 31\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.028\n",
      "Epoch: 32\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.043\n",
      "Epoch: 33\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.030\n",
      "Epoch: 34\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.032\n",
      "Epoch: 35\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.032\n",
      "Epoch: 36\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.027\n",
      "Saving model ------------------------------ at loss = 0.027.\n",
      "Epoch: 37\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.047\n",
      "Epoch: 38\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.031\n",
      "Epoch: 39\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.029\n",
      "Epoch: 40\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.028\n",
      "Epoch: 41\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.038\n",
      "Epoch: 42\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.029\n",
      "Epoch: 43\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.030\n",
      "Epoch: 44\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.032\n",
      "Epoch: 45\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.027\n",
      "Saving model ------------------------------ at loss = 0.027.\n",
      "Epoch: 46\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.027\n",
      "Epoch: 47\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.026\n",
      "Saving model ------------------------------ at loss = 0.026.\n",
      "Epoch: 48\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.027\n",
      "Epoch: 49\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.033\n",
      "Epoch: 50\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.027\n",
      "Epoch: 51\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.033\n",
      "Epoch: 52\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.027\n",
      "Epoch: 53\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.030\n",
      "Epoch: 54\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.026\n",
      "Saving model ------------------------------ at loss = 0.026.\n",
      "Epoch: 55\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.028\n",
      "Epoch: 56\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.027\n",
      "Epoch: 57\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.024\n",
      "Saving model ------------------------------ at loss = 0.024.\n",
      "Epoch: 58\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.027\n",
      "Epoch: 59\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.033\n",
      "Epoch: 60\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.064\n",
      "Epoch: 61\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.031\n",
      "Epoch: 62\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.025\n",
      "Epoch: 63\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.024\n",
      "Saving model ------------------------------ at loss = 0.024.\n",
      "Epoch: 64\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.042\n",
      "Epoch: 65\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.025\n",
      "Epoch: 66\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.026\n",
      "Epoch: 67\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.034\n",
      "Epoch: 68\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.031\n",
      "Epoch: 69\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.033\n",
      "Epoch: 70\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.026\n",
      "Epoch: 71\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.024\n",
      "Epoch: 72\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.031\n",
      "Epoch: 73\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.033\n",
      "Epoch: 74\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.025\n",
      "Epoch: 75\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.032\n",
      "Epoch: 76\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.029\n",
      "Epoch: 77\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.026\n",
      "Epoch: 78\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.024\n",
      "Epoch: 79\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.023\n",
      "Saving model ------------------------------ at loss = 0.023.\n",
      "Epoch: 80\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.026\n",
      "Epoch: 81\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.024\n",
      "Epoch: 82\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.031\n",
      "Epoch: 83\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.030\n",
      "Epoch: 84\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.025\n",
      "Epoch: 85\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.030\n",
      "Epoch: 86\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.023\n",
      "Epoch: 87\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.027\n",
      "Epoch: 88\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.022\n",
      "Saving model ------------------------------ at loss = 0.022.\n",
      "Epoch: 89\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.025\n",
      "Epoch: 90\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.024\n",
      "Epoch: 91\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.025\n",
      "Epoch: 92\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 93\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.026\n",
      "Epoch: 94\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.026\n",
      "Epoch: 95\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.024\n",
      "Epoch: 96\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.024\n",
      "Epoch: 97\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.026\n",
      "Epoch: 98\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.027\n",
      "Epoch: 99\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.028\n",
      "Epoch: 100\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.025\n",
      "Epoch: 101\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 102\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.025\n",
      "Epoch: 103\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 104\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.023\n",
      "Epoch: 105\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.026\n",
      "Epoch: 106\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.029\n",
      "Epoch: 107\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 108\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.022\n",
      "Saving model ------------------------------ at loss = 0.022.\n",
      "Epoch: 109\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.025\n",
      "Epoch: 110\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.025\n",
      "Epoch: 111\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.022\n",
      "Saving model ------------------------------ at loss = 0.022.\n",
      "Epoch: 112\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 113\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 114\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.024\n",
      "Epoch: 115\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.022\n",
      "Epoch: 116\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 117\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 118\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.023\n",
      "Epoch: 119\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.022\n",
      "Epoch: 120\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.022\n",
      "Saving model ------------------------------ at loss = 0.022.\n",
      "Epoch: 121\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.027\n",
      "Epoch: 122\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.024\n",
      "Epoch: 123\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 124\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 125\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.028\n",
      "Epoch: 126\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.024\n",
      "Epoch: 127\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.024\n",
      "Epoch: 128\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 129\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 130\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 131\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.024\n",
      "Epoch: 132\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.026\n",
      "Epoch: 133\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.028\n",
      "Epoch: 134\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 135\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.024\n",
      "Epoch: 136\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.023\n",
      "Epoch: 137\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.026\n",
      "Epoch: 138\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 139\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 140\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Saving model ------------------------------ at loss = 0.021.\n",
      "Epoch: 141\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.027\n",
      "Epoch: 142\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.024\n",
      "Epoch: 143\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 144\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 145\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.021\n",
      "Saving model ------------------------------ at loss = 0.021.\n",
      "Epoch: 146\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 147\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.024\n",
      "Epoch: 148\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 149\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 150\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 151\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 152\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 153\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 154\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.022\n",
      "Epoch: 155\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 156\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 157\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 158\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 159\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 160\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 161\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Saving model ------------------------------ at loss = 0.020.\n",
      "Epoch: 162\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 163\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 164\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.020\n",
      "Saving model ------------------------------ at loss = 0.020.\n",
      "Epoch: 165\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.027\n",
      "Epoch: 166\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 167\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 168\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 169\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.024\n",
      "Epoch: 170\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 171\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 172\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.019\n",
      "Saving model ------------------------------ at loss = 0.019.\n",
      "Epoch: 173\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 174\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 175\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.020\n",
      "Epoch: 176\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 177\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 178\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.020\n",
      "Epoch: 179\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 180\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 181\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 182\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 183\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 184\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 185\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 186\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 187\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 188\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 189\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 190\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 191\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 192\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 193\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 194\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 195\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 196\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 197\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 198\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 199\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.025\n",
      "Epoch: 200\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 201\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 202\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.024\n",
      "Epoch: 203\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.024\n",
      "Epoch: 204\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 205\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.023\n",
      "Epoch: 206\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.024\n",
      "Epoch: 207\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 208\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.027\n",
      "Epoch: 209\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 210\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 211\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 212\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 213\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 214\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 215\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 216\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 217\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 218\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.022\n",
      "Epoch: 219\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 220\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.024\n",
      "Epoch: 221\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 222\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 223\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 224\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 225\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 226\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 227\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.025\n",
      "Epoch: 228\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 229\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.026\n",
      "Epoch: 230\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 231\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 232\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 233\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 234\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 235\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 236\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.019\n",
      "Saving model ------------------------------ at loss = 0.019.\n",
      "Epoch: 237\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 238\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 239\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.026\n",
      "Epoch: 240\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 241\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 242\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.021\n",
      "Epoch: 243\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 244\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 245\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 246\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.019\n",
      "Epoch: 247\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 248\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.025\n",
      "Epoch: 249\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 250\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 251\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 252\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 253\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.023\n",
      "Epoch: 254\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.019\n",
      "Epoch: 255\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 256\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 257\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Saving model ------------------------------ at loss = 0.019.\n",
      "Epoch: 258\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 259\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 260\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 261\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 262\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 263\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.019\n",
      "Saving model ------------------------------ at loss = 0.019.\n",
      "Epoch: 264\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 265\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 266\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 267\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 268\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 269\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 270\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 271\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 272\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 273\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 274\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 275\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 276\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 277\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 278\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 279\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 280\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Saving model ------------------------------ at loss = 0.019.\n",
      "Epoch: 281\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 282\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 283\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 284\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.024\n",
      "Epoch: 285\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 286\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 287\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.024\n",
      "Epoch: 288\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.025\n",
      "Epoch: 289\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 290\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 291\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 292\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 293\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 294\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 295\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 296\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 297\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 298\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 299\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 300\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 301\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 302\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 303\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 304\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 305\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 306\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 307\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.021\n",
      "Epoch: 308\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 309\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 310\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 311\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 312\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 313\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 314\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 315\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 316\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 317\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Epoch: 318\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 319\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.024\n",
      "Epoch: 320\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 321\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 322\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 323\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.020\n",
      "Epoch: 324\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 325\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 326\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 327\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 328\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 329\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 330\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 331\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 332\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 333\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 334\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 335\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 336\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 337\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 338\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 339\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 340\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 341\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 342\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 343\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 344\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 345\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 346\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 347\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 348\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.022\n",
      "Epoch: 349\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 350\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 351\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 352\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 353\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.026\n",
      "Epoch: 354\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 355\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 356\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 357\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Saving model ------------------------------ at loss = 0.019.\n",
      "Epoch: 358\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 359\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 360\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 361\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 362\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 363\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 364\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 365\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 366\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 367\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 368\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.029\n",
      "Epoch: 369\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 370\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 371\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 372\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 373\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 374\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 375\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 376\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Epoch: 377\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 378\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 379\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 380\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 381\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 382\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 383\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 384\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 385\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Epoch: 386\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 387\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.024\n",
      "Epoch: 388\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 389\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.022\n",
      "Epoch: 390\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 391\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 392\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.025\n",
      "Epoch: 393\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.022\n",
      "Epoch: 394\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 395\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 396\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 397\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 398\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Saving model ------------------------------ at loss = 0.019.\n",
      "Epoch: 399\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 400\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 401\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.023\n",
      "Epoch: 402\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 403\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 404\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 405\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.024\n",
      "Epoch: 406\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Epoch: 407\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 408\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 409\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.026\n",
      "Epoch: 410\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 411\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 412\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 413\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 414\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.018\n",
      "Saving model ------------------------------ at loss = 0.018.\n",
      "Epoch: 415\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 416\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 417\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 418\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.022\n",
      "Epoch: 419\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 420\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.023\n",
      "Epoch: 421\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.018\n",
      "Epoch: 422\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 423\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 424\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 425\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 426\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.023\n",
      "Epoch: 427\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.026\n",
      "Epoch: 428\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.022\n",
      "Epoch: 429\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.022\n",
      "Epoch: 430\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Epoch: 431\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 432\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 433\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.022\n",
      "Epoch: 434\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Epoch: 435\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 436\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.019\n",
      "Epoch: 437\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 438\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.020\n",
      "Epoch: 439\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.022\n",
      "Epoch: 440\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.020\n",
      "Epoch: 441\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.021\n",
      "Epoch: 442\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 443\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.023\n",
      "Epoch: 444\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.021\n",
      "Epoch: 445\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.023\n",
      "Epoch: 446\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.019\n",
      "Epoch: 447\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.022\n",
      "Epoch: 448\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "score = 0\n",
    "best_score = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    # Train\n",
    "    train_score = train(model, epoch)\n",
    "    # Test\n",
    "    test_score = test(model)\n",
    "    \n",
    "    # Save best model\n",
    "    if test_score < best_score:\n",
    "        print(\"Saving model ------------------------------ at loss = {:.3f}.\".format(test_score))\n",
    "        torch.save(model.state_dict(), '{}/{}.pth'.format(LOG_PATH, EXPERIMENT_NAME))\n",
    "        best_score = test_score\n",
    "\n",
    "# Save losses\n",
    "train_loss_history = np.array(train_losses)\n",
    "np.savetxt(\"{}/{}_train_loss.txt\".format(LOG_PATH, EXPERIMENT_NAME), train_loss_history, delimiter=\",\")\n",
    "loss_history = np.array(test_losses)\n",
    "np.savetxt(\"{}/{}_test_loss.txt\".format(LOG_PATH, EXPERIMENT_NAME), loss_history, delimiter=\",\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))\n",
    "print(\"--- Time taken to train : %s mins ---\" % ((end_time - start_time)//60))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "# b, g, r, y, o, -g, -m,\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses,linewidth=4)\n",
    "plt.plot(test_losses,linewidth=4)\n",
    "#plt.title('{} loss'.format(\"Exp name\"))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train loss', 'test loss'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.savefig('{}/{}_graph.png'.format(LOG_PATH, EXPERIMENT_NAME), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
