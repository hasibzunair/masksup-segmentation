{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train COIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(0)\n",
    "os.environ['PYTHONHASHSEED'] = str(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "EXPERIMENT_NAME = \"ssl_coin2\"\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "LOG_PATH = os.path.join(ROOT_DIR, \"logs\", EXPERIMENT_NAME)\n",
    "\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)\n",
    "    \n",
    "if not os.path.exists(os.path.join(LOG_PATH, \"outs\")):\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\", \"preds\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\", \"partials\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH,\"outs\", \"inputs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hasib/context-invariance/logs/ssl_coin2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\" \n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).lower(), fontsize=20)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        #plt.tight_layout()\n",
    "    #plt.savefig(\"../outputs/vis/compare-segs/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from dataset import ISIC2018Masked_dataloader\n",
    "\n",
    "train_dataset = ISIC2018Masked_dataloader(\"datasets/ISIC2018\")\n",
    "test_dataset = ISIC2018Masked_dataloader(\"datasets/ISIC2018\", is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 192, 256]),\n",
       " torch.Size([8, 1, 192, 256]),\n",
       " torch.Size([8, 3, 192, 256]),\n",
       " torch.Size([8, 3, 192, 256]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = next(iter(train_dataloader))\n",
    "x = dt[\"image\"]\n",
    "y = dt[\"mask\"]\n",
    "z = dt[\"partial_image1\"]\n",
    "z2 = dt[\"partial_image2\"]\n",
    "x.shape, y.shape, z.shape, z2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_img(ten):\n",
    "#     ten =(ten[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     ten=(ten*255).astype(np.uint8)\n",
    "#     return ten\n",
    "\n",
    "# a = to_img(x)\n",
    "# print(a.shape)\n",
    "# #plt.imshow(a)\n",
    "# plt.imshow(a, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xs, ys, zs, zs2 in zip(x,y,z,z2):\n",
    "    \n",
    "#     p_in = (zs.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     p_in = (p_in*255).astype(np.uint8)\n",
    "    \n",
    "#     p_in2 = (zs2.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     p_in2 = (p_in2*255).astype(np.uint8)\n",
    "    \n",
    "#     # Preprocess Input\n",
    "#     xs = (xs.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     xs = (xs*255).astype(np.uint8)\n",
    "    \n",
    "#     # Preprocess Mask\n",
    "#     ys =(ys.permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     ys = (ys*255).astype(np.uint8)\n",
    "    \n",
    "#     visualize(0, input_image=xs, mask=ys, partial_input=p_in, partial_input_2=p_in2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COIN(\n",
       "  (input_block): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (input_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bridge): Bridge(\n",
       "    (bridge): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(2048, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlockForUNetWithResNet50(\n",
       "      (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block_1): ConvBlock(\n",
       "        (conv): Conv2d(67, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (conv_block_2): ConvBlock(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import * \n",
    "\n",
    "# Define model\n",
    "model = COIN()\n",
    "\n",
    "# Send to GPU and initialize weights\n",
    "model = model.to(DEVICE)\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup optim and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion_mae = torch.nn.L1Loss() # MAE, L1\n",
    "criterion_mse = torch.nn.MSELoss() # MSE, L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        data, mask, partial1, partial2 = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE), data[\"partial_image1\"].to(DEVICE), data[\"partial_image2\"].to(DEVICE)\n",
    "        \n",
    "        # Pred\n",
    "        output1, output2 = model.forward(partial1.float(), partial2.float())\n",
    "        \n",
    "        # Post process\n",
    "        output1 = output1[:,0:3,:,:]\n",
    "        output1 = torch.tanh(output1)\n",
    "        output2 = output2[:,0:3,:,:]\n",
    "        output2 = torch.tanh(output2)\n",
    "        \n",
    "        # One way loss\n",
    "        # ssl_ae\n",
    "        #loss = 0.9 * criterion_mse(output1.float(), data.float()) + 0.1 * criterion_mae(output1.float(), data.float())\n",
    "        \n",
    "        # Two way loss\n",
    "        # ssl_coin\n",
    "        loss_1 = 0.9 * criterion_mse(output1.float(), data.float()) + 0.1 * criterion_mae(output1.float(), data.float())\n",
    "        loss_2 = 0.9 * criterion_mse(output2.float(), data.float()) + 0.1 * criterion_mae(output2.float(), data.float())\n",
    "        #loss = 0.5 * loss_1 + 0.5 * loss_2\n",
    "        loss = 0.8 * loss_1 + 0.2 * loss_2\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ######################\n",
    "        if batch_idx == 0:\n",
    "            # preds\n",
    "            cv_img=(output1[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "            rgb=(cv_img*255).astype(np.uint8)\n",
    "            bgr=cv2.cvtColor(rgb,cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite('{}/outs/preds/test'.format(LOG_PATH)+str(epoch)+'.jpg',bgr)\n",
    "\n",
    "            # partial inputs\n",
    "            xx=(partial1[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "            xx=(xx*255).astype(np.uint8)\n",
    "            xx=cv2.cvtColor(xx,cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite('{}/outs/partials/test'.format(LOG_PATH)+str(epoch)+'.jpg',xx)\n",
    "\n",
    "            # inputs\n",
    "            yy=(data[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "            yy=(yy*255).astype(np.uint8)\n",
    "            yy=cv2.cvtColor(yy,cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite('{}/outs/inputs/test'.format(LOG_PATH)+str(epoch)+'.jpg',yy)\n",
    "        ######################\n",
    "        \n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "        #         100. * batch_idx / len(train_dataloader), loss.data))\n",
    "        \n",
    "    train_loss /= len(train_dataloader) \n",
    "    train_losses.append(train_loss)\n",
    "    print('Average training loss: {:.3f}'.format(train_loss))\n",
    "\n",
    "\n",
    "            \n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(train_dataloader):\n",
    "            data, mask, partial = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE), data[\"partial_image1\"].to(DEVICE)\n",
    "            \n",
    "            # Pred\n",
    "            output, _ = model.forward(partial.float(), partial.float())\n",
    "            output = output[:,0:3,:,:]\n",
    "            output = torch.tanh(output)\n",
    "            # Compute loss\n",
    "            test_loss += criterion_mse(output.float(), data.float()).item()\n",
    "        \n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print('Average test loss: {:.3f}'.format(test_loss))\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average training loss: 0.068\n",
      "Average test loss: 0.041\n",
      "Saving model ------------------------------ at loss = 0.041.\n",
      "Epoch: 2\n",
      "Average training loss: 0.043\n",
      "Average test loss: 0.071\n",
      "Epoch: 3\n",
      "Average training loss: 0.039\n",
      "Average test loss: 0.037\n",
      "Saving model ------------------------------ at loss = 0.037.\n",
      "Epoch: 4\n",
      "Average training loss: 0.036\n",
      "Average test loss: 0.085\n",
      "Epoch: 5\n",
      "Average training loss: 0.035\n",
      "Average test loss: 0.034\n",
      "Saving model ------------------------------ at loss = 0.034.\n",
      "Epoch: 6\n",
      "Average training loss: 0.033\n",
      "Average test loss: 0.050\n",
      "Epoch: 7\n",
      "Average training loss: 0.034\n",
      "Average test loss: 0.045\n",
      "Epoch: 8\n",
      "Average training loss: 0.031\n",
      "Average test loss: 0.071\n",
      "Epoch: 9\n",
      "Average training loss: 0.030\n",
      "Average test loss: 0.070\n",
      "Epoch: 10\n",
      "Average training loss: 0.030\n",
      "Average test loss: 0.053\n",
      "Epoch: 11\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.059\n",
      "Epoch: 12\n",
      "Average training loss: 0.030\n",
      "Average test loss: 0.067\n",
      "Epoch: 13\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.036\n",
      "Epoch: 14\n",
      "Average training loss: 0.029\n",
      "Average test loss: 0.126\n",
      "Epoch: 15\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.054\n",
      "Epoch: 16\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.035\n",
      "Epoch: 17\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.059\n",
      "Epoch: 18\n",
      "Average training loss: 0.028\n",
      "Average test loss: 0.064\n",
      "Epoch: 19\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.082\n",
      "Epoch: 20\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.047\n",
      "Epoch: 21\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.066\n",
      "Epoch: 22\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.077\n",
      "Epoch: 23\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.069\n",
      "Epoch: 24\n",
      "Average training loss: 0.027\n",
      "Average test loss: 0.030\n",
      "Saving model ------------------------------ at loss = 0.030.\n",
      "Epoch: 25\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.050\n",
      "Epoch: 26\n",
      "Average training loss: 0.026\n",
      "Average test loss: 0.059\n",
      "Epoch: 27\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.049\n",
      "Epoch: 28\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.051\n",
      "Epoch: 29\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.049\n",
      "Epoch: 30\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.051\n",
      "Epoch: 31\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.052\n",
      "Epoch: 32\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.063\n",
      "Epoch: 33\n",
      "Average training loss: 0.025\n",
      "Average test loss: 0.062\n",
      "Epoch: 34\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.059\n",
      "Epoch: 35\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.060\n",
      "Epoch: 36\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.066\n",
      "Epoch: 37\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.056\n",
      "Epoch: 38\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.063\n",
      "Epoch: 39\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.045\n",
      "Epoch: 40\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.065\n",
      "Epoch: 41\n",
      "Average training loss: 0.024\n",
      "Average test loss: 0.047\n",
      "Epoch: 42\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.066\n",
      "Epoch: 43\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.059\n",
      "Epoch: 44\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.054\n",
      "Epoch: 45\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.031\n",
      "Epoch: 46\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.053\n",
      "Epoch: 47\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.050\n",
      "Epoch: 48\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.066\n",
      "Epoch: 49\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.057\n",
      "Epoch: 50\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.074\n",
      "Epoch: 51\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.078\n",
      "Epoch: 52\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.066\n",
      "Epoch: 53\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.063\n",
      "Epoch: 54\n",
      "Average training loss: 0.023\n",
      "Average test loss: 0.055\n",
      "Epoch: 55\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.049\n",
      "Epoch: 56\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.057\n",
      "Epoch: 57\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.047\n",
      "Epoch: 58\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.057\n",
      "Epoch: 59\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.053\n",
      "Epoch: 60\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.068\n",
      "Epoch: 61\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.053\n",
      "Epoch: 62\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.067\n",
      "Epoch: 63\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.062\n",
      "Epoch: 64\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.070\n",
      "Epoch: 65\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.059\n",
      "Epoch: 66\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.054\n",
      "Epoch: 67\n",
      "Average training loss: 0.022\n",
      "Average test loss: 0.070\n",
      "Epoch: 68\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.063\n",
      "Epoch: 69\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.044\n",
      "Epoch: 70\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.066\n",
      "Epoch: 71\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.050\n",
      "Epoch: 72\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.047\n",
      "Epoch: 73\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.063\n",
      "Epoch: 74\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.058\n",
      "Epoch: 75\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.049\n",
      "Epoch: 76\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.040\n",
      "Epoch: 77\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.048\n",
      "Epoch: 78\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.081\n",
      "Epoch: 79\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.059\n",
      "Epoch: 80\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.044\n",
      "Epoch: 81\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.056\n",
      "Epoch: 82\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.052\n",
      "Epoch: 83\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.058\n",
      "Epoch: 84\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.051\n",
      "Epoch: 85\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.057\n",
      "Epoch: 86\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.065\n",
      "Epoch: 87\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.068\n",
      "Epoch: 88\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.056\n",
      "Epoch: 89\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.053\n",
      "Epoch: 90\n",
      "Average training loss: 0.021\n",
      "Average test loss: 0.061\n",
      "Epoch: 91\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.050\n",
      "Epoch: 92\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.052\n",
      "Epoch: 93\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.063\n",
      "Epoch: 94\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.071\n",
      "Epoch: 95\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.051\n",
      "Epoch: 96\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.059\n",
      "Epoch: 97\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.060\n",
      "Epoch: 98\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.057\n",
      "Epoch: 99\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.059\n",
      "Epoch: 100\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.060\n",
      "Epoch: 101\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.059\n",
      "Epoch: 102\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.037\n",
      "Epoch: 103\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.058\n",
      "Epoch: 104\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.054\n",
      "Epoch: 105\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.042\n",
      "Epoch: 106\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.044\n",
      "Epoch: 107\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.067\n",
      "Epoch: 108\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.059\n",
      "Epoch: 109\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.065\n",
      "Epoch: 110\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.070\n",
      "Epoch: 111\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.056\n",
      "Epoch: 112\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.058\n",
      "Epoch: 113\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.057\n",
      "Epoch: 114\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.048\n",
      "Epoch: 115\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.063\n",
      "Epoch: 116\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.074\n",
      "Epoch: 117\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.076\n",
      "Epoch: 118\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.058\n",
      "Epoch: 119\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.052\n",
      "Epoch: 120\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.054\n",
      "Epoch: 121\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.058\n",
      "Epoch: 122\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.067\n",
      "Epoch: 123\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.068\n",
      "Epoch: 124\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.059\n",
      "Epoch: 125\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.061\n",
      "Epoch: 126\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.065\n",
      "Epoch: 127\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.069\n",
      "Epoch: 128\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.055\n",
      "Epoch: 129\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.076\n",
      "Epoch: 130\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.074\n",
      "Epoch: 131\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.053\n",
      "Epoch: 132\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.062\n",
      "Epoch: 133\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.047\n",
      "Epoch: 134\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.058\n",
      "Epoch: 135\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.061\n",
      "Epoch: 136\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.058\n",
      "Epoch: 137\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.072\n",
      "Epoch: 138\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.056\n",
      "Epoch: 139\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.073\n",
      "Epoch: 140\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.051\n",
      "Epoch: 141\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.058\n",
      "Epoch: 142\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.039\n",
      "Epoch: 143\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.073\n",
      "Epoch: 144\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.064\n",
      "Epoch: 145\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.069\n",
      "Epoch: 146\n",
      "Average training loss: 0.020\n",
      "Average test loss: 0.060\n",
      "Epoch: 147\n",
      "Average training loss: 0.019\n",
      "Average test loss: 0.054\n",
      "Epoch: 148\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.041\n",
      "Epoch: 149\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.062\n",
      "Epoch: 150\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.057\n",
      "Epoch: 151\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.064\n",
      "Epoch: 152\n",
      "Average training loss: 0.018\n",
      "Average test loss: 0.069\n",
      "Epoch: 153\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d60b8de5f9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c5ce9d6182fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fifa/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fifa/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "score = 0\n",
    "best_score = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    # Train\n",
    "    train_score = train(model, epoch)\n",
    "    # Test\n",
    "    test_score = test(model)\n",
    "    \n",
    "    # Save best model\n",
    "    if test_score < best_score:\n",
    "        print(\"Saving model ------------------------------ at loss = {:.3f}.\".format(test_score))\n",
    "        torch.save(model.state_dict(), '{}/{}.pth'.format(LOG_PATH, EXPERIMENT_NAME))\n",
    "        best_score = test_score\n",
    "\n",
    "# Save losses\n",
    "train_loss_history = np.array(train_losses)\n",
    "np.savetxt(\"{}/{}_train_loss.txt\".format(LOG_PATH, EXPERIMENT_NAME), train_loss_history, delimiter=\",\")\n",
    "loss_history = np.array(test_losses)\n",
    "np.savetxt(\"{}/{}_test_loss.txt\".format(LOG_PATH, EXPERIMENT_NAME), loss_history, delimiter=\",\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))\n",
    "print(\"--- Time taken to train : %s mins ---\" % ((end_time - start_time)//60))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "# b, g, r, y, o, -g, -m,\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses,linewidth=2)\n",
    "plt.plot(test_losses,linewidth=2)\n",
    "#plt.title('{} loss'.format(\"Exp name\"))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train loss', 'test loss'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.savefig('{}/{}_graph.png'.format(LOG_PATH, EXPERIMENT_NAME), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
