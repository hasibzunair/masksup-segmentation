{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facd5880-3a77-41c8-b54c-3beecc609538",
   "metadata": {},
   "source": [
    "# Test segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c28a19-01c9-49b3-8a29-a5ad09148b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d84cd-bb50-4d06-8b91-5e2b18452f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6923a-1890-4c06-bf88-13539e0e7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "\n",
    "data_folder = 'datasets/ISIC2018'\n",
    "model_path = 'logs/unet_isic2018/unet_isic2018.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97867777-552b-4f03-b1b5-3dcc22989576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC2018_dataloader(Dataset):\n",
    "    def __init__(self, data_folder, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        self._data_folder = data_folder\n",
    "        self.build_dataset()\n",
    "\n",
    "    def build_dataset(self):\n",
    "        self._input_folder = os.path.join(self._data_folder, 'ISIC2018_Task1-2_Training_Input')\n",
    "        self._label_folder = os.path.join(self._data_folder, 'ISIC2018_Task1_Training_GroundTruth')\n",
    "        self._images = sorted(glob.glob(self._input_folder + \"/*.jpg\"))\n",
    "        self._labels = sorted(glob.glob(self._label_folder + \"/*.png\"))\n",
    "        \n",
    "        self.train_images, self.test_images, self.train_labels, self.test_labels = train_test_split(self._images, self._labels, \n",
    "                                                                            test_size=0.2, shuffle=False, random_state=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.train_images)\n",
    "        else:\n",
    "            return len(self.test_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.is_train:\n",
    "            img_path = self.train_images[idx]\n",
    "            mask_path = self.train_labels[idx]\n",
    "        else:\n",
    "            img_path = self.test_images[idx]\n",
    "            mask_path = self.test_labels[idx]\n",
    "            \n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('P')\n",
    "        \n",
    "        transforms_image = transforms.Compose([transforms.Resize((256, 256)), transforms.CenterCrop((256,256)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))])\n",
    "        \n",
    "        transforms_mask = transforms.Compose([transforms.Resize((256, 256)), transforms.CenterCrop((256,256)),\n",
    "                                             transforms.ToTensor()])\n",
    "        \n",
    "        image = transforms_image(image)\n",
    "        mask = transforms_mask(mask)\n",
    "        \n",
    "        sample = {'image': image, 'mask': mask}\n",
    "        return sample\n",
    "    \n",
    "\n",
    "test_dataset = ISIC2018_dataloader(\"datasets/ISIC2018\", is_train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144649f3-73e1-4b4a-a515-919fb1b56528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "###### UNet model\n",
    "#########################################################################################\n",
    "\"\"\" Convolutional block:\n",
    "    It follows a two 3x3 convolutional layer, each followed by a batch normalization and a relu activation.\n",
    "\"\"\"\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\"\"\" Encoder block:\n",
    "    It consists of an conv_block followed by a max pooling.\n",
    "    Here the number of filters doubles and the height and width half after every block.\n",
    "\"\"\"\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p\n",
    "\n",
    "\"\"\" Decoder block:\n",
    "    The decoder block begins with a transpose convolution, followed by a concatenation with the skip\n",
    "    connection from the encoder block. Next comes the conv_block.\n",
    "    Here the number filters decreases by half and the height and width doubles.\n",
    "\"\"\"\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "        \n",
    "        # NOTE: \n",
    "        # nn.Conv2d(64, 1, kernel_size=1, padding=0) is mathematically same as \n",
    "        # nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs\n",
    "######################################################################################### \n",
    "\n",
    "\n",
    "model = build_unet()\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c84dc0-6182-4b5d-9037-9a016900917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\" \n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).lower(), fontsize=20)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        #plt.tight_layout()\n",
    "    #plt.savefig(\"../outputs/vis/compare-segs/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051883e-9ca7-4e28-bff5-58fffafcc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataloader)\n",
    "# y[y==1] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4fa22-8680-4e24-830a-02e5b0d14b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    img, target = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE)\n",
    "    output = torch.sigmoid(model(img.float()))\n",
    "    \n",
    "    img = (img[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    img = (img*255).astype(np.uint8)\n",
    "\n",
    "    gt = target.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy()\n",
    "    pred = output.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() > 0.5\n",
    "    \n",
    "    visualize(batch_idx, input_image=img, ground_truth=gt, prediction=pred)\n",
    "    \n",
    "    if batch_idx == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e73d2-9c22-4d22-b19d-99ff877b0723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f02d4-742d-45fb-9855-4739c8bf80e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
