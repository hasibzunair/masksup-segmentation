{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facd5880-3a77-41c8-b54c-3beecc609538",
   "metadata": {},
   "source": [
    "# Test segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c28a19-01c9-49b3-8a29-a5ad09148b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d84cd-bb50-4d06-8b91-5e2b18452f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6923a-1890-4c06-bf88-13539e0e7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from dataset import GLAS_dataloader, POLYPS_dataloader, NYUDV2_dataloader\n",
    "from models.unet import build_unet\n",
    "from models.LeViTUNet128s import Build_LeViT_UNet_128s\n",
    "from models.LeViTUNet192 import Build_LeViT_UNet_192\n",
    "from models.LeViTUNet384 import Build_LeViT_UNet_384\n",
    "from models.unetplusplus import NestedUNet\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "EXPERIMENT_NAME = \"nyu_experiments/nyu_nestunet_cb_ts_h/\"\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "LOG_PATH = os.path.join(ROOT_DIR, \"logs\", EXPERIMENT_NAME)\n",
    "\n",
    "data_folder = 'datasets/NYUDV2'\n",
    "#model_path = 'logs/glas_experiments/{}/{}.pth'.format(EXPERIMENT_NAME, EXPERIMENT_NAME)\n",
    "model_path = 'logs/nyu_experiments/nyu_nestunet_cb_ts_h/nyu_nestunet_cb_ts_h.pth'\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97867777-552b-4f03-b1b5-3dcc22989576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NYUDV2_dataloader(\"datasets/NYUDV2\")\n",
    "test_dataset = NYUDV2_dataloader(\"datasets/NYUDV2\", is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144649f3-73e1-4b4a-a515-919fb1b56528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Build_LeViT_UNet_384(num_classes=1, pretrained=True)\n",
    "model = NestedUNet(num_classes=40)\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c84dc0-6182-4b5d-9037-9a016900917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\" \n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).lower(), fontsize=20)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        #plt.tight_layout()\n",
    "    #plt.savefig(\"../outputs/vis/compare-segs/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051883e-9ca7-4e28-bff5-58fffafcc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataloader)\n",
    "# y[y==1] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793dba6-d433-4669-a6ca-0c17c4789d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7938e1-17a9-4f9a-8577-a7e1b678fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(LOG_PATH, \"vis_masked\")):\n",
    "    os.mkdir(os.path.join(LOG_PATH, \"vis_masked\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH, \"vis_masked\", \"imgs\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH, \"vis_masked\", \"gts\"))\n",
    "    os.mkdir(os.path.join(LOG_PATH, \"vis_masked\", \"preds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa535c9e-bd12-4ac9-a8ca-814bbd64b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load color map\n",
    "cmap = np.load('datasets/NYUDV2/cmap.npy')\n",
    "\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    img, target = data[\"partial_image1\"].to(DEVICE), data[\"mask\"].to(DEVICE) # partial_image1 instead of image\n",
    "    output = torch.sigmoid(model(img.float()))\n",
    "    \n",
    "    img = (img[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    img = (img*255).astype(np.uint8)\n",
    "    #img= cv2.cvtColor(img,cv2.COLOR_RGB2BGR) # uncomment when saving images\n",
    "    gt = target.squeeze().data.cpu().numpy()\n",
    "    gt = cmap[gt]\n",
    "    \n",
    "    output = torch.softmax(output, dim=1).argmax(dim=1)[0].float().cpu().numpy().astype(np.uint8)\n",
    "    pred = cmap[output]\n",
    "    \n",
    "    visualize(batch_idx, input_image=img, ground_truth=gt, prediction=pred)\n",
    "    \n",
    "    if batch_idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b4e6-1444-4310-b982-aef5577bc4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a01735e5-df16-48df-b724-f17f3aa898bc",
   "metadata": {},
   "source": [
    "### Save masked images and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4fa22-8680-4e24-830a-02e5b0d14b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load color map\n",
    "cmap = np.load('datasets/NYUDV2/cmap.npy')\n",
    "\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    img, target = data[\"partial_image1\"].to(DEVICE), data[\"mask\"].to(DEVICE) # partial_image1 instead of image\n",
    "    output = torch.sigmoid(model(img.float()))\n",
    "    \n",
    "    img = (img[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    img = (img*255).astype(np.uint8)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     gt = target.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy()\n",
    "#     gt=(gt*255).astype(np.uint8)\n",
    "#     gt=cv2.cvtColor(gt,cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "#     pred = output.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() > 0.5\n",
    "#     pred=(pred*255).astype(np.uint8)\n",
    "#     pred=cv2.cvtColor(pred,cv2.COLOR_RGB2BGR)\n",
    "    gt = target.squeeze().data.cpu().numpy()\n",
    "    gt = cmap[gt]\n",
    "\n",
    "    # pred = (\n",
    "    #     cv2.resize(\n",
    "    #         output[0, :40].data.cpu().numpy().transpose(1, 2, 0),\n",
    "    #         target.size()[1:][::-1],\n",
    "    #         interpolation=cv2.INTER_CUBIC,\n",
    "    #     )\n",
    "    #     .argmax(axis=2)\n",
    "    #     .astype(np.uint8)\n",
    "    # )\n",
    "\n",
    "    output = torch.softmax(output, dim=1).argmax(dim=1)[0].float().cpu().numpy().astype(np.uint8)\n",
    "    pred = cmap[output]\n",
    "    \n",
    "    cv2.imwrite(os.path.join(LOG_PATH, \"vis_masked\", \"imgs/\")+str(batch_idx)+'.png', img)\n",
    "    cv2.imwrite(os.path.join(LOG_PATH, \"vis_masked\", \"gts/\")+str(batch_idx)+'.png', gt)\n",
    "    cv2.imwrite(os.path.join(LOG_PATH, \"vis_masked\", \"preds/\")+str(batch_idx)+'.png', pred)\n",
    "    \n",
    "    \n",
    "    #visualize(batch_idx, input_image=img, ground_truth=gt, prediction=pred)\n",
    "    \n",
    "    #if batch_idx == 20:\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71331168-1cb6-4978-a0e5-e12ca535c2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
