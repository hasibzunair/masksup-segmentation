{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facd5880-3a77-41c8-b54c-3beecc609538",
   "metadata": {},
   "source": [
    "# Generalization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c28a19-01c9-49b3-8a29-a5ad09148b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d84cd-bb50-4d06-8b91-5e2b18452f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d895bb77-2581-4e7c-9f1c-428ceb02f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.cuda.is_available = lambda : False\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535a152-ae67-4710-a1b8-775516c830a3",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feeaec0-4a31-459c-a72a-4f20eb51e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class polyp_test_dataloader(Dataset):\n",
    "    \"\"\"\n",
    "    KVASIR-Seg data loader\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        self._data_folder = data_folder\n",
    "        self.build_dataset()\n",
    "\n",
    "    def build_dataset(self):\n",
    "        self._input_folder = os.path.join(self._data_folder, 'images')\n",
    "        self._label_folder = os.path.join(self._data_folder, 'masks')\n",
    "        self._images = glob.glob(self._input_folder + \"/*.png\")\n",
    "        self._labels = glob.glob(self._label_folder + \"/*.png\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self._images[idx]\n",
    "        mask_path = self._labels[idx]\n",
    "        \n",
    "        # Read image, mask and scribble\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        mask[mask<=127] = 0\n",
    "        mask[mask>127] = 1\n",
    "        mask = cv2.resize(mask, (224, 224), interpolation = cv2.INTER_AREA)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "        transforms_image = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                               transforms.CenterCrop((224,224)),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "        transforms_mask = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                              transforms.CenterCrop((224,224)),\n",
    "                                              transforms.ToTensor()])\n",
    "        \n",
    "        # Conver to torch tensors\n",
    "        image = transforms_image(image)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        \n",
    "        sample = {'image': image, \n",
    "                  'mask': mask\n",
    "                 }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9907f1d-556b-46a8-9bb2-7637f8a32e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_img(ten):\n",
    "#     ten =(ten[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     ten=(ten*255).astype(np.uint8)\n",
    "#     return ten\n",
    "\n",
    "# a = to_img(x)\n",
    "# print(a.shape)\n",
    "# plt.imshow(a)\n",
    "# #plt.imshow(a, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c8f707-9060-4640-9e73-98162d57230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = to_img(y)\n",
    "# print(a.shape)\n",
    "# plt.imshow(a, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc0fcf-9f91-4414-83fd-ad87a1ab1764",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a28462-0441-4fd9-b1c5-6456dfedef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.kiunet import unet\n",
    "from models.LeViTUNet128s import Build_LeViT_UNet_128s\n",
    "from models.LeViTUNet192 import Build_LeViT_UNet_192\n",
    "from models.LeViTUNet384 import Build_LeViT_UNet_384\n",
    "\n",
    "#cvc_model_cb_ts_e/h\n",
    "\n",
    "EXPERIMENT_NAME = \"polys_levit384_cb_h\"\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "LOG_PATH = os.path.join(ROOT_DIR, \"logs\", EXPERIMENT_NAME)\n",
    "model_path = 'logs/{}/{}.pth'.format(EXPERIMENT_NAME, EXPERIMENT_NAME)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb512246-89f2-4e25-8325-4f8ec4712055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = unet()\n",
    "#model = Build_LeViT_UNet_128s(num_classes=1, pretrained=True)\n",
    "#model = Build_LeViT_UNet_192(num_classes=1, pretrained=True)\n",
    "model = Build_LeViT_UNet_384(num_classes=1, pretrained=True)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b6208-33cb-482a-a51d-1af486a95287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metric_percase\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        jaccard = 0\n",
    "        dice = 0\n",
    "        for data_name in ['CVC-ClinicDB', 'Kvasir', 'CVC-300', 'CVC-ColonDB', 'ETIS-LaribPolypDB']:\n",
    "            test_dataset = polyp_test_dataloader(\"datasets/POLYPS/TestDataset/\"+data_name)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "            for data in test_dataloader:\n",
    "                data, target = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE)\n",
    "                output = model(data.float())\n",
    "                dc, jc, _ = calculate_metric_percase(output, target)\n",
    "                jaccard += jc\n",
    "                dice += dc\n",
    "            jaccard /= len(test_dataloader)\n",
    "            dice /= len(test_dataloader)\n",
    "            print(f\"Scores for {data_name}\")\n",
    "            print(f\"Jaccard Index / IoU : {jaccard*100:.3f}\")\n",
    "            print(f\"Dice Coeff / F1 : {dice*100}\")\n",
    "            #print('Jaccard Index / IoU : {:.3f}'.format(jaccard * 100))\n",
    "            #print('Dice Coefficient / F1 : {:.3f}'.format(dice * 100))\n",
    "            print('==========================================')\n",
    "            print('==========================================')\n",
    "        return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e4f89-ccdb-4e4b-8e8e-5fb7d2e5d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_score = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3317692-f815-4477-b8dd-ce017170f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "if not os.path.exists(os.path.join(LOG_PATH, \"vis_test\")):\n",
    "    os.mkdir(os.path.join(LOG_PATH, \"vis_test\"))\n",
    "    for data_name in ['CVC-ClinicDB', 'Kvasir', 'CVC-300', 'CVC-ColonDB', 'ETIS-LaribPolypDB']:\n",
    "        os.mkdir(os.path.join(LOG_PATH, \"vis_test\", data_name))\n",
    "        os.mkdir(os.path.join(LOG_PATH, \"vis_test\", data_name, \"imgs\"))\n",
    "        os.mkdir(os.path.join(LOG_PATH, \"vis_test\", data_name, \"gts\"))\n",
    "        os.mkdir(os.path.join(LOG_PATH, \"vis_test\", data_name, \"preds\"))\n",
    "\n",
    "for data_name in ['CVC-ClinicDB', 'Kvasir', 'CVC-300', 'CVC-ColonDB', 'ETIS-LaribPolypDB']:\n",
    "    test_dataset = polyp_test_dataloader(\"datasets/POLYPS/TestDataset/\"+data_name)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "    for batch_idx, data in enumerate(test_dataloader):\n",
    "        img, target = data[\"image\"].to(DEVICE), data[\"mask\"].to(DEVICE)\n",
    "        output = torch.sigmoid(model(img.float()))\n",
    "\n",
    "        img = (img[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "        img = (img*255).astype(np.uint8)\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        gt = target.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy()\n",
    "        gt=(gt*255).astype(np.uint8)\n",
    "        gt=cv2.cvtColor(gt,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        pred = output.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() > 0.5\n",
    "        pred=(pred*255).astype(np.uint8)\n",
    "        pred=cv2.cvtColor(pred,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        cv2.imwrite(os.path.join(LOG_PATH, \"vis_test\", data_name, \"imgs/\")+str(batch_idx)+'.png', img)\n",
    "        cv2.imwrite(os.path.join(LOG_PATH, \"vis_test\", data_name, \"gts/\")+str(batch_idx)+'.png', gt)\n",
    "        cv2.imwrite(os.path.join(LOG_PATH, \"vis_test\", data_name, \"preds/\")+str(batch_idx)+'.png', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da33585-cba7-407e-9962-9a150cd1d3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acaccd2-4e0e-4973-9488-00491585609d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
